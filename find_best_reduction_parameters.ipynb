{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/mtsizh/galaxy-morphology-manifold-learning/blob/main/find_best_reduction_parameters.ipynb",
      "authorship_tag": "ABX9TyN3Nt3vYssaQCg6npn5myhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fbeilstein/algorithms/blob/master/find_best_reduction_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you performed dataset curation on your own - upload `curated_imgs.zip` and skip to the next step. Otherwise you can run the following code and download the curated dataseet from GitHub."
      ],
      "metadata": {
        "id": "o0rLKfHa__4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/mtsizh/galaxy-morphology-manifold-learning/main/curated_dataset/curated_imgs_multipart.zip && echo \"HEAD dowloaded\" || \"ERROR downloading HEAD\"\n",
        "\n",
        "for i in range(1,8):\n",
        "  !wget -q https://raw.githubusercontent.com/mtsizh/galaxy-morphology-manifold-learning/main/curated_dataset/curated_imgs_multipart.z0{i}  && echo \"PART {i} of 7 OK\" || \"ERROR downloading PART {i}\"\n",
        "\n",
        "print('MERGING PARTS')\n",
        "!zip -FF curated_imgs_multipart.zip --out curated_imgs.zip > /dev/null && rm curated_imgs_multipart.z* && echo \"COMPLETE\" || \"FAILED\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peMXLfox98FW",
        "outputId": "7c205074-fb12-40dd-e896-51ca57a24cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD dowloaded\n",
            "PART 1 of 7 OK\n",
            "PART 2 of 7 OK\n",
            "PART 3 of 7 OK\n",
            "PART 4 of 7 OK\n",
            "PART 5 of 7 OK\n",
            "PART 6 of 7 OK\n",
            "PART 7 of 7 OK\n",
            "MERGING PARTS\n",
            "COMPLETE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the curated dataset."
      ],
      "metadata": {
        "id": "fmqRwqWhAZux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o curated_imgs.zip && echo \"UNZIPPED\" || \"FAIL\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efbdUckc_4PW",
        "outputId": "1baac1e7-7ce2-4a3f-fcb4-a9e4f08b69e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNZIPPED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few libraries are not installed by default. the following code installs `optuna` and `umap-learn`."
      ],
      "metadata": {
        "id": "JraRV82hAe4l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqO98zoY9Qv7",
        "outputId": "695cf813-7995-498f-c1e8-78acd8574034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPLETE\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "\n",
        "try:\n",
        "  import optuna\n",
        "  from cuml.manifold import TSNE\n",
        "  from cuml.manifold import UMAP\n",
        "  from cuml.decomposition import PCA\n",
        "  from google.colab import output\n",
        "  output.clear()\n",
        "except:\n",
        "  print('ERROR')\n",
        "finally:\n",
        "  print('COMPLETE')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to generate a report on different methods. `optuna` is used to get the best parameters for each of the methods: t-SNE, uMap, IsoMap, LLE, PCA. Result is saved in form of a `json` file. See variable `reports_folder` for path.\n",
        "\n",
        "|File|Purpose|\n",
        "|---|---|\n",
        "|`results.json`|Best parameters for each method|\n",
        "`log_intermediate_results.json`|All results tested by `optuna` library|\n",
        "|images.npy|Images to use the same batch for all methods even after runtime disconnection|\n",
        "|classes.npy|Classes for images to use the same batch for all methods even after runtime disconnection|\n",
        "\n",
        "**Warning** Colab may periodically turn off the virtual machine. To avoid data loss after each dimensionality reduction method is processed results are stored to json files. If these files are present on the next run, methods already fully processed are skipped. For more data safety you may want to set `reports_folder` to Google Drive path so that it is not deleted if the runtime is removed."
      ],
      "metadata": {
        "id": "Oqjs29XhAtRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import json\n",
        "import pprint\n",
        "import cupy as cp\n",
        "from cuml.manifold import TSNE\n",
        "from cuml.manifold import UMAP\n",
        "from cuml.decomposition import PCA\n",
        "import warnings\n",
        "from sklearn.manifold import LocallyLinearEmbedding, Isomap\n",
        "import os.path\n",
        "\n",
        "\n",
        "# use different class maps to get different estimations\n",
        "class_map = {1: 'round', 2: 'inbetween', 3: 'cigar'}\n",
        "#class_map = {4: 'edge on', 5: 'edge off'}\n",
        "#class_map = {6: 'smooth', 7: 'featured'}\n",
        "methods = ['t-SNE', 'uMap', 'LLE', 'Isomap', 'PCA']\n",
        "n_bootstrap_samples = 5000\n",
        "n_parameter_trials = 50\n",
        "reports_folder = './drive/MyDrive' #\n",
        "best_resuts_file = os.path.join(reports_folder, 'results.json')\n",
        "all_results_file = os.path.join(reports_folder, 'log_intermediate_results.json')\n",
        "images_arr_file = os.path.join(reports_folder, 'images.npy')\n",
        "classes_arr_file = os.path.join(reports_folder, 'classes.npy')\n",
        "\n",
        "\n",
        "def load_images():\n",
        "  df = pd.read_parquet('curated_dataset.parquet')\n",
        "  regex_filter = '|'.join(class_map.values())\n",
        "  filtered_df = df[df['class'].str.contains(regex_filter, regex=True)]\n",
        "  bootstrapped_df = filtered_df.sample(n=n_bootstrap_samples, random_state=25)\n",
        "  X = np.zeros((len(bootstrapped_df), 120, 120))\n",
        "  y = np.zeros(len(bootstrapped_df))\n",
        "\n",
        "  for key, val in class_map.items():\n",
        "    y[bootstrapped_df['class'].str.contains(val, regex=True)] = key\n",
        "\n",
        "  print('Dataset balance:')\n",
        "  for k,v in class_map.items():\n",
        "    print(f'class {v} has {np.sum(y == k)} items')\n",
        "  print('-----------------------------------------')\n",
        "\n",
        "  print('LOAD IMAGES')\n",
        "  paths = bootstrapped_df['png_loc'].str.replace('dr5', 'curated_imgs')\n",
        "  with tqdm(total=len(paths)) as progress:\n",
        "    for idx, file_path in enumerate(paths):\n",
        "      with Image.open(file_path) as img:\n",
        "        X[idx,:,:] = np.array(img)\n",
        "        progress.update()\n",
        "  np.save(images_arr_file, X)\n",
        "  np.save(classes_arr_file, y)\n",
        "  return X, y\n",
        "\n",
        "\n",
        "if os.path.isfile(images_arr_file) and os.path.isfile(classes_arr_file):\n",
        "  X = np.load(images_arr_file)\n",
        "  y = np.load(classes_arr_file)\n",
        "else:\n",
        "  X, y = load_images()\n",
        "\n",
        "X_flattened = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# load previously calculated results if present\n",
        "log_intermediate_results = []\n",
        "result = []\n",
        "if (os.path.isfile(best_resuts_file) and\n",
        "    os.path.isfile(all_results_file)):\n",
        "  with open(best_resuts_file) as json_file:\n",
        "    result = json.load(json_file)\n",
        "  with open(all_results_file) as json_file:\n",
        "    log_intermediate_results = json.load(json_file)\n",
        "\n",
        "\n",
        "def objective(trial, methods):\n",
        "  global log_intermediate_results\n",
        "  dr_method = trial.suggest_categorical('dr_method', methods)\n",
        "\n",
        "  if dr_method == 't-SNE':\n",
        "    n_components = 2 # cuml supports only 2 components, sklearn tsne works 3 days\n",
        "    perplexity = trial.suggest_int('perplexity', 5, min(50, X.shape[0]-1)) # perplexity < samples\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 3*(perplexity+1), max(3*(perplexity+1), min(50, X.shape[0]-1)))\n",
        "    reducer = TSNE(n_components=n_components, perplexity=perplexity,\n",
        "                   method='fft', n_neighbors=n_neighbors)\n",
        "    log_intermediate_results.append({'dr_method': 't-SNE', 'perplexity': perplexity, 'n_neighbors': n_neighbors})\n",
        "  elif dr_method == 'LLE':\n",
        "    n_components = trial.suggest_int('n_components', 2, min(200, X.shape[0]-1)) # components < samples\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', min(2, n_components),\n",
        "                                    max(50, n_components)) # neighbors <= samples\n",
        "    reducer = LocallyLinearEmbedding(n_components=n_components, n_neighbors=n_neighbors, n_jobs=-1)\n",
        "    log_intermediate_results.append({'dr_method': 'LLE', 'n_components': n_components, 'n_neighbors': n_neighbors})\n",
        "  elif dr_method == 'Isomap':\n",
        "    n_components = trial.suggest_int('n_components', 2, min(200, X.shape[0]-1)) # components < samples\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 10, min(50, X.shape[0]//2))\n",
        "    reducer = Isomap(n_components=n_components, n_neighbors=n_neighbors, n_jobs=-1)\n",
        "    log_intermediate_results.append({'dr_method': 'Isomap', 'n_components': n_components, 'n_neighbors': n_neighbors})\n",
        "  elif dr_method == 'PCA':\n",
        "    n_components = trial.suggest_int('n_components', 2, np.min([200, X.shape[0]-1, X.shape[1]-1]))\n",
        "    reducer = PCA(n_components=n_components, svd_solver='full')\n",
        "    log_intermediate_results.append({'dr_method': 'PCA', 'n_components': n_components})\n",
        "  elif dr_method == 'uMap':\n",
        "    n_components = trial.suggest_int('n_components', 2, np.min([200, X.shape[0]//2, X.shape[1]-1]))\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 5, min(50, X.shape[0]-1)) # neighbors < samples\n",
        "    reducer = UMAP(n_neighbors=n_neighbors, n_components=n_components)\n",
        "    log_intermediate_results.append({'dr_method': 'UMAP', 'n_components': n_components, 'n_neighbors': n_neighbors})\n",
        "\n",
        "  try: #ISOMAP fails without any reason\n",
        "    X_reduced = reducer.fit_transform(X_flattened)\n",
        "  except ValueError as e:\n",
        "    print(f\"Skipping {method} trial due to error: {e}\")\n",
        "    log_intermediate_results = log_intermediate_results[:-1]\n",
        "    return -np.inf # not to spoil the result\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(),\n",
        "                      LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
        "  quality = np.mean(cross_val_score(clf, X_reduced, y, cv=5))\n",
        "  log_intermediate_results[-1]['quality'] = quality\n",
        "\n",
        "  return quality\n",
        "\n",
        "# ignore annoying futurewarnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*force_all_finite.*\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*default method of TSNE.*\", category=UserWarning)\n",
        "\n",
        "methods_processed = [x['dr_method'] for x in result]\n",
        "for method in methods:\n",
        "  print(f'********************************{method}***************************')\n",
        "  if method in methods_processed:\n",
        "    print('Already present in JSON file as processed, skipping')\n",
        "    continue\n",
        "  study = optuna.create_study(direction=\"maximize\")\n",
        "  study.optimize(lambda T: objective(T, [method]),\n",
        "                n_trials=n_parameter_trials, show_progress_bar=True, n_jobs=1)\n",
        "  print(\"Best parameters:\", study.best_params, \"Best value:\", study.best_value)\n",
        "  result.append(study.best_params)\n",
        "  result[-1]['best_vavlue'] = study.best_value\n",
        "\n",
        "  pretty_json_str = pprint.pformat(result, compact=True).replace(\"'\",'\"')\n",
        "  with open(best_resuts_file, \"w\") as outfile:\n",
        "      outfile.write(pretty_json_str)\n",
        "\n",
        "  pretty_json_str = pprint.pformat(log_intermediate_results, compact=True).replace(\"'\",'\"')\n",
        "  with open(all_results_file, \"w\") as outfile:\n",
        "      outfile.write(pretty_json_str)"
      ],
      "metadata": {
        "id": "5_aBDQbSMlFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZE839kJ7OrKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}